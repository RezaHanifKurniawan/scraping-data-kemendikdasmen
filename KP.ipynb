{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73af22a",
   "metadata": {},
   "source": [
    "#  Scraper SD/MI Kabupaten Semarang  \n",
    "### Mengambil Data Profil Sekolah dari Kemendikbud (Referensi + Sekolah.Data)\n",
    "\n",
    "Project ini bertujuan untuk melakukan pengambilan data sekolah tingkat **SD** dan **MI** di Kabupaten Semarang secara otomatis dari dua sumber resmi Kemendikbud untuk penyediaan data guna mendukung marketing canvasing pada Fams Medika Holistic Care Center:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94a0b62",
   "metadata": {},
   "source": [
    "##  **Sumber Data**\n",
    "\n",
    "1. **referensi.data.kemendikdasmen.go.id**  \n",
    "   - Menyediakan *listing* sekolah berdasarkan kecamatan  \n",
    "   - Data yang diambil:\n",
    "     - Nama Sekolah  \n",
    "     - NPSN  \n",
    "     - Status (Negeri/Swasta)  \n",
    "     - Kelurahan  \n",
    "     - Link menuju halaman referensi sekolah (dipakai untuk mendapatkan UUID)\n",
    "\n",
    "2. **sekolah.data.kemendikdasmen.go.id (Profil Sekolah)**  \n",
    "   - Menyediakan detail profil setiap sekolah  \n",
    "   - Data yang diambil:\n",
    "     - Alamat  \n",
    "     - Kepala Sekolah  \n",
    "     - Telepon  \n",
    "     - Email  \n",
    "     - Website  \n",
    "     - Jumlah Siswa Laki-laki  \n",
    "     - Jumlah Siswa Perempuan  \n",
    "\n",
    "---\n",
    "\n",
    "##  **Metode Pengambilan Data**\n",
    "\n",
    "### 1️ **Listing Sekolah (UC Driver)**  \n",
    "Listing sekolah diambil menggunakan **undetected_chromedriver (UC Driver)** untuk menghindari deteksi bot dan memastikan halaman referensi dapat di-*render* dengan stabil.  \n",
    "Hanya **1 driver UC** yang digunakan sepanjang proses listing untuk efisiensi.\n",
    "\n",
    "---\n",
    "\n",
    "##  **Metode Versi 1 — Selenium Full (Tanpa ThreadPool)**  \n",
    "Versi ini:\n",
    "- Menggunakan **1 UC driver** untuk listing  \n",
    "- Menggunakan **1 standard Selenium driver** untuk setiap sekolah  \n",
    "- Semua detail diambil **secara sequential** (satu per satu)  \n",
    "- Keunggulan: lebih stabil  \n",
    "- Kekurangan: waktu scraping sangat lama karena setiap sekolah diproses berurutan\n",
    "\n",
    "---\n",
    "\n",
    "## **Metode Versi 2 — Optimized Parallel Detail Scraper (ThreadPool)**  \n",
    "Versi optimasi ini:\n",
    "- **Listing tetap UC driver (1 instance)**  \n",
    "- **Detail sekolah diambil paralel** menggunakan:\n",
    "  - `ThreadPoolExecutor(max_workers=N)`\n",
    "  - Setiap worker menggunakan **Selenium standard driver** terpisah\n",
    "- UUID untuk setiap sekolah tetap diperoleh menggunakan `requests` + `BeautifulSoup`\n",
    "- Keunggulan: **sangat cepat**, bisa 5–10x lebih cepat dari versi v1  \n",
    "- Kekurangan: memerlukan resource CPU & RAM lebih besar\n",
    "\n",
    "---\n",
    "\n",
    "##  Output\n",
    "Hasil scraping disimpan dalam file CSV di folder:\n",
    "output/list_sd_mi_{nama_kecamatan}.csv\n",
    "\n",
    "\n",
    "Kolom CSV dapat dipilih sesuai kebutuhan (Nama Sekolah, NPSN, Alamat, Kepala Sekolah, dst).\n",
    "\n",
    "---\n",
    "\n",
    "##  Catatan\n",
    "- UC Driver digunakan untuk *listing* agar tidak terblokir  \n",
    "- Standard Selenium digunakan untuk halaman detail karena lebih stabil di Angular frontend  \n",
    "- `requests + BeautifulSoup` dipakai hanya untuk mem-parsing halaman referensi lama (HTML statis) guna mengambil UUID dengan cepat tanpa Selenium  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaefa73",
   "metadata": {},
   "source": [
    "## Install Library yang dipakai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd2feb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.38.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting urllib3<3.0,>=2.5.0 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
      "  Downloading trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
      "  Using cached trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2025.10.5 (from selenium)\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting typing_extensions<5.0,>=4.15.0 (from selenium)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting websocket-client<2.0,>=1.8.0 (from selenium)\n",
      "  Using cached websocket_client-1.9.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting attrs>=23.2.0 (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sortedcontainers (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting idna (from trio<1.0,>=0.31.0->selenium)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached cffi-2.0.0-cp310-cp310-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting exceptiongroup (from trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached exceptiongroup-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
      "  Downloading wsproto-1.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3.0,>=2.5.0->selenium)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio<1.0,>=0.31.0->selenium)\n",
      "  Using cached pycparser-2.23-py3-none-any.whl.metadata (993 bytes)\n",
      "Collecting h11<1,>=0.16.0 (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading selenium-4.38.0-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.6/9.7 MB 9.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.4/9.7 MB 6.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.4/9.7 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.7/9.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.0/9.7 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.3/9.7 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.7 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 6.2 MB/s  0:00:01\n",
      "Downloading trio-0.32.0-py3-none-any.whl (512 kB)\n",
      "Using cached trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Using cached cffi-2.0.0-cp310-cp310-win_amd64.whl (182 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.3.1-py3-none-any.whl (24 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, websocket-client, urllib3, typing_extensions, sniffio, pysocks, pycparser, idna, h11, certifi, attrs, wsproto, outcome, exceptiongroup, cffi, trio, trio-websocket, selenium\n",
      "\n",
      "   -- -------------------------------------  1/18 [websocket-client]\n",
      "   ---- -----------------------------------  2/18 [urllib3]\n",
      "   ------------- --------------------------  6/18 [pycparser]\n",
      "   --------------- ------------------------  7/18 [idna]\n",
      "   ---------------------- ----------------- 10/18 [attrs]\n",
      "   ------------------------------- -------- 14/18 [cffi]\n",
      "   --------------------------------- ------ 15/18 [trio]\n",
      "   --------------------------------- ------ 15/18 [trio]\n",
      "   --------------------------------- ------ 15/18 [trio]\n",
      "   --------------------------------- ------ 15/18 [trio]\n",
      "   ------------------------------------- -- 17/18 [selenium]\n",
      "   ------------------------------------- -- 17/18 [selenium]\n",
      "   ------------------------------------- -- 17/18 [selenium]\n",
      "   ------------------------------------- -- 17/18 [selenium]\n",
      "   ------------------------------------- -- 17/18 [selenium]\n",
      "   ------------------------------------- -- 17/18 [selenium]\n",
      "   ------------------------------------- -- 17/18 [selenium]\n",
      "   ------------------------------------- -- 17/18 [selenium]\n",
      "   ------------------------------------- -- 17/18 [selenium]\n",
      "   ---------------------------------------- 18/18 [selenium]\n",
      "\n",
      "Successfully installed attrs-25.4.0 certifi-2025.11.12 cffi-2.0.0 exceptiongroup-1.3.0 h11-0.16.0 idna-3.11 outcome-1.3.0.post0 pycparser-2.23 pysocks-1.7.1 selenium-4.38.0 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.32.0 trio-websocket-0.12.2 typing_extensions-4.15.0 urllib3-2.5.0 websocket-client-1.9.0 wsproto-1.3.1\n",
      "Collecting undetected-chromedriver\n",
      "  Using cached undetected_chromedriver-3.5.5-py3-none-any.whl\n",
      "Requirement already satisfied: selenium>=4.9.0 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from undetected-chromedriver) (4.38.0)\n",
      "Collecting requests (from undetected-chromedriver)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting websockets (from undetected-chromedriver)\n",
      "  Using cached websockets-15.0.1-cp310-cp310-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium>=4.9.0->undetected-chromedriver) (2.5.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2025.11.12)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (1.9.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (3.11)\n",
      "Requirement already satisfied: outcome in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (2.0.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium>=4.9.0->undetected-chromedriver) (1.3.1)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.31.0->selenium>=4.9.0->undetected-chromedriver) (2.23)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium>=4.9.0->undetected-chromedriver) (0.16.0)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->undetected-chromedriver)\n",
      "  Downloading charset_normalizer-3.4.4-cp310-cp310-win_amd64.whl.metadata (38 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp310-cp310-win_amd64.whl (107 kB)\n",
      "Using cached websockets-15.0.1-cp310-cp310-win_amd64.whl (176 kB)\n",
      "Installing collected packages: websockets, charset_normalizer, requests, undetected-chromedriver\n",
      "\n",
      "   ---------------------------------------- 0/4 [websockets]\n",
      "   -------------------- ------------------- 2/4 [requests]\n",
      "   ---------------------------------------- 4/4 [undetected-chromedriver]\n",
      "\n",
      "Successfully installed charset_normalizer-3.4.4 requests-2.32.5 undetected-chromedriver-3.5.5 websockets-15.0.1\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Using cached beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Using cached soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "\n",
      "   ---------------------------------------- 2/2 [beautifulsoup4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.14.2 soupsieve-2.8\n",
      "Collecting gradio\n",
      "  Using cached gradio-5.49.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio)\n",
      "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting brotli>=1.1.0 (from gradio)\n",
      "  Downloading brotli-1.2.0-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.121.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-1.0.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.13.3 (from gradio)\n",
      "  Using cached gradio_client-1.13.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Using cached groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting httpx<1.0,>=0.24.1 (from gradio)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.33.5 (from gradio)\n",
      "  Downloading huggingface_hub-1.1.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting jinja2<4.0 (from gradio)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting markupsafe<4.0,>=2.0 (from gradio)\n",
      "  Using cached markupsafe-3.0.3-cp310-cp310-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting numpy<3.0,>=1.0 (from gradio)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.11.4-cp310-cp310-win_amd64.whl.metadata (42 kB)\n",
      "Collecting packaging (from gradio)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pandas<3.0,>=1.0 (from gradio)\n",
      "  Using cached pandas-2.3.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pillow<12.0,>=8.0 (from gradio)\n",
      "  Using cached pillow-11.3.0-cp310-cp310-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting pydantic<2.12,>=2.0 (from gradio)\n",
      "  Using cached pydantic-2.11.10-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pyyaml<7.0,>=5.0 (from gradio)\n",
      "  Using cached pyyaml-6.0.3-cp310-cp310-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.14.5-py3-none-win_amd64.whl.metadata (26 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.7-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Using cached tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from gradio) (4.15.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Using cached uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting fsspec (from gradio-client==1.13.3->gradio)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: websockets<16.0,>=13.0 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.49.3-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi<1.0,>=0.115.2->gradio)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
      "Collecting httpcore==1.* (from httpx<1.0,>=0.24.1->gradio)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\asus\\miniconda3\\envs\\kp\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Collecting filelock (from huggingface-hub<2.0,>=0.33.5->gradio)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.33.5->gradio)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.33.5->gradio)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub<2.0,>=0.33.5->gradio)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.33.5->gradio)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas<3.0,>=1.0->gradio)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas<3.0,>=1.0->gradio)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3.0,>=1.0->gradio)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Using cached pydantic_core-2.33.2-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting colorama (from click>=8.0.0->typer<1.0,>=0.12->gradio)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio)\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Using cached pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached gradio-5.49.1-py3-none-any.whl (63.5 MB)\n",
      "Using cached gradio_client-1.13.3-py3-none-any.whl (325 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Downloading fastapi-0.121.2-py3-none-any.whl (109 kB)\n",
      "Using cached groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading huggingface_hub-1.1.4-py3-none-any.whl (515 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 2.4/2.9 MB 13.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 12.1 MB/s  0:00:00\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached markupsafe-3.0.3-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Downloading orjson-3.11.4-cp310-cp310-win_amd64.whl (131 kB)\n",
      "Using cached pandas-2.3.3-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "Using cached pillow-11.3.0-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "Using cached pydantic-2.11.10-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "Using cached pyyaml-6.0.3-cp310-cp310-win_amd64.whl (158 kB)\n",
      "Downloading safehttpx-0.1.7-py3-none-any.whl (9.0 kB)\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.49.3-py3-none-any.whl (74 kB)\n",
      "Using cached tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading brotli-1.2.0-cp310-cp310-win_amd64.whl (369 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading ruff-0.14.5-py3-none-win_amd64.whl (14.3 MB)\n",
      "   ---------------------------------------- 0.0/14.3 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 2.1/14.3 MB 10.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.9/14.3 MB 10.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 5.5/14.3 MB 9.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.6/14.3 MB 9.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 10.0/14.3 MB 9.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.1/14.3 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.3 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.3/14.3 MB 9.9 MB/s  0:00:01\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading ffmpy-1.0.0-py3-none-any.whl (5.6 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: pytz, pydub, brotli, tzdata, typing-inspection, tomlkit, six, shellingham, semantic-version, ruff, pyyaml, python-multipart, pygments, pydantic-core, pillow, packaging, orjson, numpy, mdurl, markupsafe, httpcore, hf-xet, groovy, fsspec, filelock, ffmpy, colorama, annotated-types, annotated-doc, aiofiles, tqdm, python-dateutil, pydantic, markdown-it-py, jinja2, click, anyio, uvicorn, typer-slim, starlette, rich, pandas, httpx, typer, safehttpx, huggingface-hub, fastapi, gradio-client, gradio\n",
      "\n",
      "   ----------------------------------------  0/49 [pytz]\n",
      "   ----------------------------------------  0/49 [pytz]\n",
      "    ---------------------------------------  1/49 [pydub]\n",
      "   -- -------------------------------------  3/49 [tzdata]\n",
      "   -- -------------------------------------  3/49 [tzdata]\n",
      "   -- -------------------------------------  3/49 [tzdata]\n",
      "   ---- -----------------------------------  5/49 [tomlkit]\n",
      "   ------- --------------------------------  9/49 [ruff]\n",
      "   -------- ------------------------------- 10/49 [pyyaml]\n",
      "   --------- ------------------------------ 12/49 [pygments]\n",
      "   --------- ------------------------------ 12/49 [pygments]\n",
      "   --------- ------------------------------ 12/49 [pygments]\n",
      "   --------- ------------------------------ 12/49 [pygments]\n",
      "   --------- ------------------------------ 12/49 [pygments]\n",
      "   --------- ------------------------------ 12/49 [pygments]\n",
      "   --------- ------------------------------ 12/49 [pygments]\n",
      "   --------- ------------------------------ 12/49 [pygments]\n",
      "   --------- ------------------------------ 12/49 [pygments]\n",
      "   --------- ------------------------------ 12/49 [pygments]\n",
      "   ----------- ---------------------------- 14/49 [pillow]\n",
      "   ----------- ---------------------------- 14/49 [pillow]\n",
      "   ----------- ---------------------------- 14/49 [pillow]\n",
      "   ----------- ---------------------------- 14/49 [pillow]\n",
      "   ------------- -------------------------- 16/49 [orjson]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   ------------- -------------------------- 17/49 [numpy]\n",
      "   -------------- ------------------------- 18/49 [mdurl]\n",
      "   ---------------- ----------------------- 20/49 [httpcore]\n",
      "   ------------------ --------------------- 23/49 [fsspec]\n",
      "   ------------------ --------------------- 23/49 [fsspec]\n",
      "   --------------------- ------------------ 26/49 [colorama]\n",
      "   ------------------------ --------------- 30/49 [tqdm]\n",
      "   ------------------------- -------------- 31/49 [python-dateutil]\n",
      "   -------------------------- ------------- 32/49 [pydantic]\n",
      "   -------------------------- ------------- 32/49 [pydantic]\n",
      "   -------------------------- ------------- 32/49 [pydantic]\n",
      "   -------------------------- ------------- 33/49 [markdown-it-py]\n",
      "   --------------------------- ------------ 34/49 [jinja2]\n",
      "   ---------------------------- ----------- 35/49 [click]\n",
      "   ----------------------------- ---------- 36/49 [anyio]\n",
      "   ------------------------------ --------- 37/49 [uvicorn]\n",
      "   ------------------------------- -------- 38/49 [typer-slim]\n",
      "   ------------------------------- -------- 39/49 [starlette]\n",
      "   -------------------------------- ------- 40/49 [rich]\n",
      "   -------------------------------- ------- 40/49 [rich]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   --------------------------------- ------ 41/49 [pandas]\n",
      "   ----------------------------------- ---- 43/49 [typer]\n",
      "   ------------------------------------ --- 45/49 [huggingface-hub]\n",
      "   ------------------------------------ --- 45/49 [huggingface-hub]\n",
      "   ------------------------------------ --- 45/49 [huggingface-hub]\n",
      "   ------------------------------------ --- 45/49 [huggingface-hub]\n",
      "   ------------------------------------- -- 46/49 [fastapi]\n",
      "   ------------------------------------- -- 46/49 [fastapi]\n",
      "   -------------------------------------- - 47/49 [gradio-client]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------  48/49 [gradio]\n",
      "   ---------------------------------------- 49/49 [gradio]\n",
      "\n",
      "Successfully installed aiofiles-24.1.0 annotated-doc-0.0.4 annotated-types-0.7.0 anyio-4.11.0 brotli-1.2.0 click-8.3.1 colorama-0.4.6 fastapi-0.121.2 ffmpy-1.0.0 filelock-3.20.0 fsspec-2025.10.0 gradio-5.49.1 gradio-client-1.13.3 groovy-0.1.2 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-1.1.4 jinja2-3.1.6 markdown-it-py-4.0.0 markupsafe-3.0.3 mdurl-0.1.2 numpy-2.2.6 orjson-3.11.4 packaging-25.0 pandas-2.3.3 pillow-11.3.0 pydantic-2.11.10 pydantic-core-2.33.2 pydub-0.25.1 pygments-2.19.2 python-dateutil-2.9.0.post0 python-multipart-0.0.20 pytz-2025.2 pyyaml-6.0.3 rich-14.2.0 ruff-0.14.5 safehttpx-0.1.7 semantic-version-2.10.0 shellingham-1.5.4 six-1.17.0 starlette-0.49.3 tomlkit-0.13.3 tqdm-4.67.1 typer-0.20.0 typer-slim-0.20.0 typing-inspection-0.4.2 tzdata-2025.2 uvicorn-0.38.0\n"
     ]
    }
   ],
   "source": [
    "! pip install selenium\n",
    "! pip install undetected-chromedriver\n",
    "! pip install requests\n",
    "! pip install beautifulsoup4\n",
    "! pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf0ff2",
   "metadata": {},
   "source": [
    "##  Ambil List Kode dan Nama Kecamatan\n",
    "\n",
    "Tahapan ini berfungsi untuk **mengambil dan memetakan daftar kode serta nama kecamatan** di Kabupaten Semarang, yang kemudian digunakan dalam dua fungsi utama sistem:\n",
    "\n",
    "1. **Sebagai dasar penyusunan URL dinamis untuk proses scraping**, dan  \n",
    "2. **Sebagai sumber data untuk dropdown pemilihan kecamatan pada tahap antarmuka (deployment)**\n",
    "\n",
    "---\n",
    "Setiap halaman daftar sekolah pada situs resmi **Kemendikdasmen** mengikuti pola URL yang tetap:\n",
    "https://referensi.data.kemendikdasmen.go.id/pendidikan/dikdas/{kode_kecamatan}/3\n",
    "\n",
    "Bagian `{kode_kecamatan}` menunjukkan **identitas numerik unik** untuk setiap kecamatan.  \n",
    "Contoh:\n",
    "- Kecamatan *Ambarawa* memiliki kode `032210`,  \n",
    "\n",
    "Untuk mendapatkan kode ini secara sistematis, sistem membaca file JSON lokal (misalnya `kecamatan_kab_semarang.json`) yang berisi pasangan:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Kab. Semarang\": {\n",
    "    \"kode\": \"032200\",\n",
    "    \"kecamatan\": {\n",
    "      \"Ambarawa\": \"032210\",\n",
    "      \"Tengaran\": \"032202\",\n",
    "      \"Susukan\": \"032203\",\n",
    "      ....\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d3a9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengambil daftar kabupaten/kota di Provinsi Jawa Tengah...\n",
      "\n",
      "Daftar Kabupaten/Kota di Jawa Tengah:\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'030100 - Kab. Cilacap'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'030200 - Kab. Banyumas'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'030300 - Kab. Purbalingga'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'030400 - Kab. Banjarnegara'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'030500 - Kab. Kebumen'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'030600 - Kab. Purworejo'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'030700 - Kab. Wonosobo'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'030800 - Kab. Magelang'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'030900 - Kab. Boyolali'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'031000 - Kab. Klaten'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'031100 - Kab. Sukoharjo'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'031200 - Kab. Wonogiri'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'031300 - Kab. Karanganyar'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'031400 - Kab. Sragen'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'031500 - Kab. Grobogan'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'031600 - Kab. Blora'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'031700 - Kab. Rembang'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'031800 - Kab. Pati'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'031900 - Kab. Kudus'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'032000 - Kab. Jepara'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'032100 - Kab. Demak'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'032200 - Kab. Semarang'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'032300 - Kab. Temanggung'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'032400 - Kab. Kendal'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'032500 - Kab. Batang'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'032600 - Kab. Pekalongan'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'032700 - Kab. Pemalang'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'032800 - Kab. Tegal'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'032900 - Kab. Brebes'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'036000 - Kota Magelang'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'036100 - Kota Surakarta'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'036200 - Kota Salatiga'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'036300 - Kota Semarang'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'036400 - Kota Pekalongan'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'036500 - Kota Tegal'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "Mengambil daftar kecamatan di Kab. Semarang...\n",
      "Berhasil ambil 19 kecamatan dari Kab. Semarang\n",
      "Data disimpan ke: data\\kecamatan_kab_semarang.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "def setup_driver(headless=True):\n",
    "    options = Options()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    # options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--log-level=3\")\n",
    "    options.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "    options.page_load_strategy = \"eager\"\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def get_table_rows(driver, url):\n",
    "    \"\"\"Helper untuk ambil semua baris dari tabel referensi\"\"\"\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"table#table1 tbody tr\"))\n",
    "    )\n",
    "    try:\n",
    "        select = Select(driver.find_element(By.NAME, \"table1_length\"))\n",
    "        select.select_by_value(\"100\")\n",
    "    except:\n",
    "        pass\n",
    "    return driver.find_elements(By.CSS_SELECTOR, \"table#table1 tbody tr\")\n",
    "\n",
    "\n",
    "def get_kecamatan_jateng_by_kode():\n",
    "    \"\"\"\n",
    "    Ambil daftar kabupaten/kota di Jawa Tengah (030000),\n",
    "    input kode kabupaten/kota (misal 032200),\n",
    "    hasil disimpan ke data/kecamatan_<nama_kab>.json\n",
    "    dalam format JSON hierarkis.\n",
    "    \"\"\"\n",
    "    base_url = \"https://referensi.data.kemendikdasmen.go.id/pendidikan/dikdas/\"\n",
    "    driver = setup_driver(headless=True)\n",
    "    kecamatan_list = []\n",
    "\n",
    "    try:\n",
    "        # 1️⃣ Ambil daftar kabupaten/kota di Provinsi Jawa Tengah\n",
    "        print(\"Mengambil daftar kabupaten/kota di Provinsi Jawa Tengah...\")\n",
    "        kab_url = base_url + \"030000/1\"\n",
    "        rows_kab = get_table_rows(driver, kab_url)\n",
    "\n",
    "        kabupaten_list = []\n",
    "        for r in rows_kab:\n",
    "            try:\n",
    "                nama = r.find_element(By.CSS_SELECTOR, \"td:nth-child(2)\").text.strip()\n",
    "                href = r.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "                kode = href.split(\"/\")[-2]\n",
    "                kabupaten_list.append({\"nama\": nama, \"kode\": kode})\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(\"\\nDaftar Kabupaten/Kota di Jawa Tengah:\")\n",
    "        print(\"=\" * 60)\n",
    "        for k in kabupaten_list:\n",
    "            display(f\"{k['kode']} - {k['nama']}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        kode_input = input(\"\\nMasukkan KODE kabupaten/kota yang ingin discrap (contoh: 032200): \").strip()\n",
    "\n",
    "        kab = next((k for k in kabupaten_list if k[\"kode\"] == kode_input), None)\n",
    "        if not kab:\n",
    "            print(\"Kode tidak ditemukan dalam daftar.\")\n",
    "            driver.quit()\n",
    "            return []\n",
    "\n",
    "        print(f\"\\nMengambil daftar kecamatan di {kab['nama']}...\")\n",
    "\n",
    "        # 2️⃣ Ambil daftar kecamatan dari kabupaten/kota terpilih\n",
    "        kec_url = base_url + f\"{kab['kode']}/2\"\n",
    "        rows_kec = get_table_rows(driver, kec_url)\n",
    "\n",
    "        for r in rows_kec:\n",
    "            try:\n",
    "                nama = r.find_element(By.CSS_SELECTOR, \"td:nth-child(2)\").text.strip()\n",
    "                href = r.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "                kode = href.split(\"/\")[-2]\n",
    "                kecamatan_list.append({\"nama\": nama, \"kode\": kode, \"url\": href})\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(f\"Berhasil ambil {len(kecamatan_list)} kecamatan dari {kab['nama']}\")\n",
    "\n",
    "        # 3️⃣ Simpan hasil ke list_kecamatan/\n",
    "        os.makedirs(\"list_kecamatan\", exist_ok=True)\n",
    "        safe_name = kab[\"nama\"].replace(\" \", \"_\").replace(\".\", \"\").lower()\n",
    "        save_path = os.path.join(\"list_kecamatan\", f\"kecamatan_{safe_name}.json\")\n",
    "\n",
    "        # === Format sesuai contoh Prof ===\n",
    "        json_data = {\n",
    "            kab[\"nama\"]: {\n",
    "                \"kode\": kab[\"kode\"],\n",
    "                \"kecamatan\": {k[\"nama\"]: k[\"kode\"] for k in kecamatan_list}\n",
    "            }\n",
    "        }\n",
    "\n",
    "        with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"Data disimpan ke: {save_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi kesalahan: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    return kecamatan_list\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# MAIN\n",
    "# ==========================\n",
    "if __name__ == \"__main__\":\n",
    "    kecamatan_data = get_kecamatan_jateng_by_kode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb0bf4",
   "metadata": {},
   "source": [
    "# GRADIO DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d7006",
   "metadata": {},
   "source": [
    "##  Versi 1 — Selenium Full (Tanpa Parallel / ThreadPool)\n",
    "\n",
    "###  Ringkasan\n",
    "Versi pertama dari scraper ini menggunakan pendekatan **Selenium murni** untuk seluruh proses pengambilan data — baik listing sekolah maupun halaman detail. Semua proses berjalan **secara sequential** (satu per satu), sehingga memiliki stabilitas tinggi namun waktu scraping relatif lama.\n",
    "\n",
    "---\n",
    "\n",
    "##  Sumber Data\n",
    "\n",
    "1. **referensi.data.kemendikdasmen.go.id**  \n",
    "   Digunakan untuk mengambil:\n",
    "   - Nama Sekolah  \n",
    "   - NPSN  \n",
    "   - Status  \n",
    "   - Kelurahan  \n",
    "   - URL menuju detail sekolah (masih format HTML lama)\n",
    "\n",
    "2. **sekolah.data.kemendikdasmen.go.id (Profil Sekolah)**  \n",
    "   Digunakan untuk mengambil data profil lengkap:\n",
    "   - Alamat  \n",
    "   - Kepala Sekolah  \n",
    "   - Telepon  \n",
    "   - Email  \n",
    "   - Website  \n",
    "   - Jumlah Siswa Laki-laki  \n",
    "   - Jumlah Siswa Perempuan  \n",
    "\n",
    "---\n",
    "\n",
    "##  Teknologi yang Digunakan\n",
    "\n",
    "### 1️ **Undetected ChromeDriver (UC Driver) — untuk Listing**  \n",
    "- Dipakai untuk membuka halaman listing sekolah  \n",
    "- Lebih tahan terhadap blocking dari Cloudflare / security check  \n",
    "- Hanya 1 driver digunakan sepanjang proses listing\n",
    "\n",
    "### 2️ **Standard Selenium Chrome Driver — untuk Detail Sekolah**  \n",
    "- Setiap sekolah dibuka menggunakan Selenium Chrome standar  \n",
    "- Lebih stabil dalam memuat halaman Angular di `sekolah.data.kemendikdasmen.go.id`  \n",
    "- Dipanggil secara sequential\n",
    "\n",
    "### 3️ **BeautifulSoup + Requests (minimal)**  \n",
    "- Hanya digunakan untuk parsing HTML referensi (untuk mendapatkan UUID)\n",
    "- Halaman referensi lama lebih ringan sehingga tidak perlu Selenium\n",
    "\n",
    "---\n",
    "\n",
    "##  Metode Pengambilan Data\n",
    "\n",
    "### **A. Listing Sekolah (1 UC Driver)**\n",
    "- Membuka halaman referensi berdasarkan kecamatan\n",
    "- Mengatur tampilan ke 100 baris\n",
    "- Mengumpulkan informasi dasar dari tabel\n",
    "- Mengambil URL referensi detail setiap sekolah\n",
    "\n",
    "### **B. Ambil UUID (requests + BeautifulSoup)**\n",
    "- Mengakses halaman referensi lama\n",
    "- Mencari link `profil-sekolah/{uuid}`\n",
    "- UUID ini digunakan untuk mengakses halaman profil baru\n",
    "\n",
    "### **C. Ambil Detail Sekolah (Selenium Full Sequential)**\n",
    "Untuk setiap sekolah:\n",
    "1. Membuka halaman profil sekolah  \n",
    "2. Menunggu elemen tertentu muncul (Angular rendering)  \n",
    "3. Mengambil seluruh data profil  \n",
    "4. Menutup driver  \n",
    "5. Melanjutkan ke sekolah berikutnya  \n",
    "\n",
    "Semua dilakukan **tanpa parallel**, sehingga lebih aman namun lambat.\n",
    "\n",
    "---\n",
    "\n",
    "##  Estimasi Waktu Scraping (Versi 1)\n",
    "Estimasi berdasarkan 1 kecamatan:\n",
    "\n",
    "| Jumlah Sekolah | Waktu (perkiraan) |\n",
    "|----------------|------------------|\n",
    "| 20 sekolah     | 2 – 4 menit |\n",
    "| 40 sekolah     | 5 – 8 menit |\n",
    "| 60 sekolah     | 9 – 14 menit |\n",
    "| 80 sekolah     | 12 – 18 menit |\n",
    "\n",
    "Faktor yang mempengaruhi:\n",
    "- Kecepatan internet  \n",
    "- Performa CPU (Chrome headless per halaman)  \n",
    "- Waktu render Angular (halaman profil sekolah cukup berat)  \n",
    "\n",
    "Versi ini **sangat stabil**, tetapi waktu eksekusi cukup panjang karena setiap profil sekolah membuka driver baru dan menunggu render.\n",
    "\n",
    "---\n",
    "\n",
    "##  Kelebihan & Kekurangan\n",
    "\n",
    "###  Kelebihan\n",
    "- Paling stabil  \n",
    "- Hampir tidak pernah error timeout  \n",
    "- Mudah di-deploy ke server  \n",
    "- Tidak memerlukan resource tinggi  \n",
    "\n",
    "###  Kekurangan\n",
    "- Sangat lambat untuk kecamatan dengan >50 sekolah  \n",
    "- Banyak instance Chrome dibuat dan dihancurkan satu per satu  \n",
    "- Tidak ideal untuk scraping massal (20 kecamatan sekaligus)  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b83fe402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium (standard + uc fallback)\n",
    "try:\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    STANDARD_SELENIUM_AVAILABLE = True\n",
    "except:\n",
    "    STANDARD_SELENIUM_AVAILABLE = False\n",
    "\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "import logging\n",
    "uc.logger.setLevel(logging.ERROR)\n",
    "\n",
    "import gradio as gr\n",
    "from time import sleep\n",
    "\n",
    "# ============================ CONFIG ============================\n",
    "HEADLESS = True\n",
    "TIMEOUT_PAGE = 15\n",
    "# ================================================================\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  UC DRIVER → hanya untuk LISTING\n",
    "# =====================================================\n",
    "def setup_uc_driver(headless=True):\n",
    "    opts = uc.ChromeOptions()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--window-size=1600,900\")\n",
    "    opts.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--log-level=3\")\n",
    "\n",
    "    try:\n",
    "        driver = uc.Chrome(options=opts)\n",
    "    except:\n",
    "        driver = setup_standard_driver(headless=headless)\n",
    "\n",
    "    driver.set_page_load_timeout(TIMEOUT_PAGE)\n",
    "    return driver\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  STANDARD SELENIUM → dipakai hanya untuk DETAIL\n",
    "# =====================================================\n",
    "def setup_standard_driver(headless=True):\n",
    "    try:\n",
    "        opts = ChromeOptions()\n",
    "        if headless:\n",
    "            opts.add_argument(\"--headless=new\")\n",
    "\n",
    "        opts.add_argument(\"--no-sandbox\")\n",
    "        opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "        opts.add_argument(\"--window-size=1600,900\")\n",
    "        opts.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "        opts.add_argument(\"--disable-gpu\")\n",
    "        opts.add_argument(\"--log-level=3\")\n",
    "        opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "        driver = webdriver.Chrome(options=opts)\n",
    "        driver.set_page_load_timeout(TIMEOUT_PAGE)\n",
    "        return driver\n",
    "    except:\n",
    "        return setup_uc_driver(headless=headless)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  REQUEST FAST SESSION\n",
    "# =====================================================\n",
    "def create_fast_session():\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    return s\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  READ JSON\n",
    "# =====================================================\n",
    "def get_kode_kecamatan_from_json(nama_kecamatan, json_path=\"./list_kecamatan/kecamatan_kab_semarang.json\"):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    wilayah = next(iter(data))\n",
    "    return data[wilayah][\"kecamatan\"].get(nama_kecamatan)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  GET UUID FROM REFERENSI (HTML lama)\n",
    "# =====================================================\n",
    "def extract_uuid_from_referensi(url, session):\n",
    "    try:\n",
    "        r = session.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        a = soup.find(\"a\", href=lambda x: x and \"profil-sekolah\" in x)\n",
    "        if a:\n",
    "            return a[\"href\"].rstrip(\"/\").split(\"/\")[-1]\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  DETAIL SCRAPER (NO THREADPOOL)\n",
    "# =====================================================\n",
    "def fetch_detail_single(href, base):\n",
    "    session = create_fast_session()\n",
    "    uuid = extract_uuid_from_referensi(href, session)\n",
    "    if not uuid:\n",
    "        return base\n",
    "\n",
    "    url = f\"https://sekolah.data.kemendikdasmen.go.id/profil-sekolah/{uuid}\"\n",
    "\n",
    "    driver = setup_standard_driver(headless=HEADLESS)\n",
    "\n",
    "    detail = {\n",
    "        \"Alamat\": \"-\",\n",
    "        \"Kepala Sekolah\": \"-\",\n",
    "        \"Telepon\": \"-\",\n",
    "        \"Email\": \"-\",\n",
    "        \"Website\": \"-\",\n",
    "        \"Jumlah Siswa Laki-laki\": \"-\",\n",
    "        \"Jumlah Siswa Perempuan\": \"-\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # alamat\n",
    "        try:\n",
    "            WebDriverWait(driver, TIMEOUT_PAGE).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"h1 + p\"))\n",
    "            )\n",
    "            detail[\"Alamat\"] = driver.find_element(By.CSS_SELECTOR, \"h1 + p\").text.strip()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # blok info\n",
    "        try:\n",
    "            blocks = driver.find_elements(By.CSS_SELECTOR, \"div.grid div.flex\")\n",
    "            for blk in blocks:\n",
    "                try:\n",
    "                    label = blk.find_element(By.CSS_SELECTOR, \".text-slate-500\").text.lower().strip()\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                if \"kepala sekolah\" in label:\n",
    "                    detail[\"Kepala Sekolah\"] = blk.find_element(By.CSS_SELECTOR, \".font-semibold\").text.strip()\n",
    "\n",
    "                elif \"telepon\" in label:\n",
    "                    try:\n",
    "                        detail[\"Telepon\"] = blk.find_element(By.TAG_NAME, \"a\").text.strip()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif \"email\" in label:\n",
    "                    try:\n",
    "                        detail[\"Email\"] = blk.find_element(By.TAG_NAME, \"a\").text.strip()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif \"website\" in label:\n",
    "                    try:\n",
    "                        detail[\"Website\"] = blk.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                    except:\n",
    "                        pass\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # statistik\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"section div.grid\"))\n",
    "            )\n",
    "            stat_blocks = driver.find_elements(By.CSS_SELECTOR, \"section div.grid div.flex\")\n",
    "\n",
    "            for blk in stat_blocks:\n",
    "                try:\n",
    "                    lbl = blk.find_element(By.CSS_SELECTOR, \"div.text-slate-600\").text.lower()\n",
    "                    val = blk.find_element(By.CSS_SELECTOR, \"div.text-2xl\").text.strip()\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                if \"laki\" in lbl or \"lak\" in lbl or \"pa\" in lbl:\n",
    "                    detail[\"Jumlah Siswa Laki-laki\"] = val\n",
    "\n",
    "                if \"perempuan\" in lbl or \"putri\" in lbl:\n",
    "                    detail[\"Jumlah Siswa Perempuan\"] = val\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"DETAIL ERROR:\", e)\n",
    "\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return {**base, **detail}\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  MAIN SCRAPER (NO THREADPOOL)\n",
    "# =====================================================\n",
    "def get_sd_mi_schools_final(kode_kecamatan, nama_kecamatan, selected_fields):\n",
    "\n",
    "    list_driver = setup_uc_driver(headless=HEADLESS)\n",
    "\n",
    "    sekolah_list = []\n",
    "\n",
    "    need_detail = any(f in selected_fields for f in [\n",
    "        \"Alamat\", \"Kepala Sekolah\", \"Telepon\", \"Email\", \"Website\",\n",
    "        \"Jumlah Siswa Laki-laki\", \"Jumlah Siswa Perempuan\"\n",
    "    ])\n",
    "\n",
    "    # ============= LIST SEKOLAH =============\n",
    "    for jenjang, value in [(\"SD\", \"5\"), (\"MI\", \"9\")]:\n",
    "        url = f\"https://referensi.data.kemendikdasmen.go.id/pendidikan/dikdas/{kode_kecamatan}/3/all/{value}/all\"\n",
    "\n",
    "        list_driver.get(url)\n",
    "\n",
    "        WebDriverWait(list_driver, 15).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"table#table1 tbody tr\"))\n",
    "        )\n",
    "\n",
    "        # 100 rows\n",
    "        try:\n",
    "            Select(list_driver.find_element(By.NAME, \"table1_length\")).select_by_value(\"100\")\n",
    "            sleep(0.3)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        rows = list_driver.find_elements(By.CSS_SELECTOR, \"table#table1 tbody tr\")\n",
    "\n",
    "        for r in rows:\n",
    "            data = {}\n",
    "\n",
    "            if \"Nama Sekolah\" in selected_fields:\n",
    "                data[\"Nama Sekolah\"] = r.find_element(By.CSS_SELECTOR, \"td:nth-child(3)\").text.strip()\n",
    "            if \"NPSN\" in selected_fields:\n",
    "                data[\"NPSN\"] = r.find_element(By.CSS_SELECTOR, \"td:nth-child(2)\").text.strip()\n",
    "            if \"Status\" in selected_fields:\n",
    "                data[\"Status\"] = r.find_element(By.CSS_SELECTOR, \"td:nth-child(6)\").text.strip()\n",
    "            if \"Kelurahan\" in selected_fields:\n",
    "                data[\"Kelurahan\"] = r.find_element(By.CSS_SELECTOR, \"td:nth-child(5)\").text.strip()\n",
    "\n",
    "            # DETAIL (no parallel)\n",
    "            if need_detail:\n",
    "                href = r.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\")\n",
    "                full_data = fetch_detail_single(href, data)\n",
    "                sekolah_list.append(full_data)\n",
    "            else:\n",
    "                sekolah_list.append(data)\n",
    "\n",
    "    try:\n",
    "        list_driver.quit()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # SORT\n",
    "    if selected_fields:\n",
    "        sort_key = selected_fields[0]\n",
    "        sekolah_list.sort(key=lambda x: str(x.get(sort_key, \"\")).lower())\n",
    "\n",
    "    return sekolah_list\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  SAVE CSV\n",
    "# =====================================================\n",
    "def save_school_list_by_kecamatan(nama_kecamatan, selected_fields):\n",
    "    kode = get_kode_kecamatan_from_json(nama_kecamatan)\n",
    "    data = get_sd_mi_schools_final(kode, nama_kecamatan, selected_fields)\n",
    "\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    path = f\"output/list_sd_mi_{nama_kecamatan.replace(' ', '_').lower()}.csv\"\n",
    "\n",
    "    all_cols = [\n",
    "        \"Kelurahan\", \"Nama Sekolah\", \"NPSN\", \"Status\",\n",
    "        \"Kepala Sekolah\", \"Alamat\", \"Telepon\", \"Email\", \"Website\",\n",
    "        \"Jumlah Siswa Laki-laki\", \"Jumlah Siswa Perempuan\"\n",
    "    ]\n",
    "    cols = [c for c in all_cols if c in selected_fields]\n",
    "\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=cols)\n",
    "        w.writeheader()\n",
    "        w.writerows(data)\n",
    "\n",
    "    return f\"{len(data)} sekolah disimpan ke {path}\", path\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  GRADIO\n",
    "# =====================================================\n",
    "def run_scraper(nama_kecamatan, selected_fields):\n",
    "    if nama_kecamatan == \"-- Pilih Kecamatan --\":\n",
    "        return \"Pilih kecamatan.\", gr.update(visible=False)\n",
    "\n",
    "    if not selected_fields:\n",
    "        return \"Pilih minimal 1 kolom.\", gr.update(visible=False)\n",
    "\n",
    "    start = time.time()\n",
    "    status, path = save_school_list_by_kecamatan(nama_kecamatan, selected_fields)\n",
    "    dur = int(time.time() - start)\n",
    "\n",
    "    m, s = divmod(dur, 60)\n",
    "    waktu = f\"\\nWaktu: {m} menit {s} detik\" if m else f\"\\nWaktu: {s} detik\"\n",
    "\n",
    "    return status + waktu, gr.update(value=path, visible=True)\n",
    "\n",
    "\n",
    "def create_gradio_ui(json_path=\"./list_kecamatan/kecamatan_kab_semarang.json\"):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    wilayah = next(iter(d))\n",
    "    kec_list = sorted(d[wilayah][\"kecamatan\"].keys())\n",
    "\n",
    "    fields = [\n",
    "        \"Kelurahan\", \"Nama Sekolah\", \"NPSN\", \"Status\",\n",
    "        \"Kepala Sekolah\", \"Alamat\", \"Telepon\", \"Email\", \"Website\",\n",
    "        \"Jumlah Siswa Laki-laki\", \"Jumlah Siswa Perempuan\"\n",
    "    ]\n",
    "\n",
    "    with gr.Blocks(title=\"Scraper SD/MI Kabupaten Semarang - No ThreadPool\") as demo:\n",
    "        gr.Markdown(\"## Scraper SD/MI Kabupaten Semarang — Selenium Full (Tanpa ThreadPool)\")\n",
    "\n",
    "        kec = gr.Dropdown(choices=[\"-- Pilih Kecamatan --\"] + kec_list,\n",
    "                          value=\"-- Pilih Kecamatan --\",\n",
    "                          label=\"Pilih Kecamatan\")\n",
    "\n",
    "        kolom = gr.CheckboxGroup(label=\"Kolom CSV\", choices=fields)\n",
    "\n",
    "        btn = gr.Button(\"Mulai Scrape\")\n",
    "        status = gr.Textbox(label=\"Status\", lines=5)\n",
    "        file = gr.File(label=\"CSV Output\", visible=False)\n",
    "\n",
    "        btn.click(run_scraper, [kec, kolom], [status, file], queue=False)\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "ui = create_gradio_ui()\n",
    "ui.launch(inbrowser=True, share=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ec4f8",
   "metadata": {},
   "source": [
    "##  Versi 2 — Hybrid Optimized (UC + Selenium + ThreadPool)\n",
    "\n",
    "###  Ringkasan\n",
    "Versi 2 adalah **peningkatan performa besar** dari versi 1.  \n",
    "Jika pada versi 1 semua proses dilakukan *secara sequential*, maka pada versi 2 dilakukan optimasi dengan:\n",
    "\n",
    "- Tetap memakai **UC Driver** untuk *listing sekolah* (agar anti-blocking dan stabil)\n",
    "- Menggunakan **Selenium Chrome Driver standar** untuk *scraping detail* (lebih stabil di Angular)\n",
    "- Menjalankan pengambilan detail secara **parallel (ThreadPoolExecutor)**  \n",
    "  → Sehingga scraping menjadi **5–10× lebih cepat** dibanding versi pertama.\n",
    "\n",
    "Versi ini dirancang untuk performa tinggi, tetap stabil, dan cocok digunakan untuk scraping massal dalam jumlah besar.\n",
    "\n",
    "---\n",
    "\n",
    "##  Sumber Data\n",
    "\n",
    "1. **referensi.data.kemendikdasmen.go.id**  \n",
    "   Diambil via UC Driver  \n",
    "   - Nama Sekolah  \n",
    "   - NPSN  \n",
    "   - Status  \n",
    "   - Kelurahan  \n",
    "   - URL referensi detail sekolah  \n",
    "\n",
    "2. **sekolah.data.kemendikdasmen.go.id (Profil Sekolah)**  \n",
    "   Diambil via Selenium Standard (parallel)  \n",
    "   - Alamat  \n",
    "   - Kepala Sekolah  \n",
    "   - Telepon  \n",
    "   - Email  \n",
    "   - Website  \n",
    "   - Jumlah Siswa Laki-laki  \n",
    "   - Jumlah Siswa Perempuan  \n",
    "\n",
    "3. **BeautifulSoup + Requests**  \n",
    "   - Untuk mengambil UUID dari halaman referensi lama  \n",
    "   - Proses ringan & cepat → tidak memerlukan Selenium  \n",
    "\n",
    "---\n",
    "\n",
    "##  Teknologi yang Digunakan\n",
    "\n",
    "### 1️ **Undetected ChromeDriver (UC Driver) – Listing**\n",
    "- Dipakai hanya untuk men-scrape tabel sekolah\n",
    "- Anti bot detection\n",
    "- Hanya 1 instance sepanjang proses\n",
    "\n",
    "### 2️ **Selenium ChromeDriver Standard – Detail (Parallel)**\n",
    "- Setiap worker menjalankan 1 instance Selenium\n",
    "- Stabil pada halaman Angular (profil sekolah)\n",
    "- Total worker dikendalikan melalui `MAX_WORKERS` (default: 12)\n",
    "\n",
    "### 3️ **ThreadPoolExecutor**\n",
    "- Menjalankan scraping detail sekolah secara paralel\n",
    "- Mempercepat eksekusi hingga 10×\n",
    "- Sangat efektif pada kecamatan dengan banyak sekolah\n",
    "\n",
    "### 4️ **Requests + BeautifulSoup**\n",
    "- Mendapatkan UUID dari halaman HTML lama\n",
    "- Beban ringan dan cepat\n",
    "- Mengurangi pemakaian Selenium\n",
    "\n",
    "---\n",
    "\n",
    "##  Metode Pengambilan Data\n",
    "\n",
    "### **A. Listing Sekolah (UC Driver)**\n",
    "- Mengambil semua sekolah dalam kecamatan (SD + MI)\n",
    "- Mengatur tabel menjadi 100 rows agar efisien\n",
    "- Menyimpan link ke halaman referensi tiap sekolah\n",
    "\n",
    "### **B. Mendapatkan UUID**\n",
    "- Menggunakan requests\n",
    "- Parsing dengan BeautifulSoup\n",
    "- Menghasilkan URL profil sekolah terbaru\n",
    "\n",
    "### **C. Detail Sekolah (Parallel Selenium Workers)**\n",
    "Untuk setiap sekolah:\n",
    "1. Mengambil URL profil  \n",
    "2. Mendaftarkan task ke ThreadPool  \n",
    "3. Masing-masing worker membuka halaman profil dengan Selenium Standard  \n",
    "4. Worker membaca:\n",
    "   - alamat  \n",
    "   - kepala sekolah  \n",
    "   - email, telepon, website  \n",
    "   - jumlah siswa laki-laki / perempuan  \n",
    "5. Data dikembalikan ke thread utama  \n",
    "6. Worker otomatis ditutup  \n",
    "\n",
    "---\n",
    "\n",
    "##  Estimasi Waktu — Versi 2 (Parallel)\n",
    "\n",
    "| Jumlah Sekolah | Estimasi Waktu |\n",
    "|----------------|----------------|\n",
    "| 20 sekolah     | 20–40 detik |\n",
    "| 40 sekolah     | 40–70 detik |\n",
    "| 60 sekolah     | 60–100 detik |\n",
    "| 80 sekolah     | 90–140 detik |\n",
    "\n",
    "Kecepatan bergantung pada:\n",
    "- Jumlah `MAX_WORKERS`\n",
    "- CPU server\n",
    "- Latency internet  \n",
    "- Waktu render Angular (profil sekolah)\n",
    "\n",
    "Dengan CPU yang bagus, versi 2 bisa bekerja **hingga 12× lebih cepat daripada versi 1**.\n",
    "\n",
    "---\n",
    "\n",
    "##  Perbandingan Versi 1 vs Versi 2\n",
    "\n",
    "| Fitur / Aspek | Versi 1 (Sequential) | Versi 2 (Parallel Optimized) |\n",
    "|--------------|----------------------|------------------------------|\n",
    "| Listing sekolah | UC Driver | UC Driver |\n",
    "| Detail sekolah | Selenium Standard (1×) | Selenium Standard + ThreadPool (multi-worker) |\n",
    "| Requests + BS4 | Ya | Ya |\n",
    "| Kecepatan | Lama (1 sekolah = 3–8 detik) | Sangat cepat (12 sekolah paralel) |\n",
    "| Stabilitas | Sangat stabil | Stabil + cepat |\n",
    "| CPU usage | Rendah | Sedang – tinggi |\n",
    "| Cocok untuk | Penggunaan kecil (1–2 kecamatan) | Scraping massal (banyak kecamatan) |\n",
    "| Penggunaan driver | 1 per sekolah | 12 per batch |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e04a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\miniconda3\\envs\\kp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Selenium imports (standard + fallback)\n",
    "try:\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    STANDARD_SELENIUM_AVAILABLE = True\n",
    "except Exception:\n",
    "    STANDARD_SELENIUM_AVAILABLE = False\n",
    "\n",
    "import undetected_chromedriver as uc  # for listing only (single instance)\n",
    "import logging\n",
    "uc.logger.setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# =================== CONFIG ===================\n",
    "MAX_WORKERS = 12  \n",
    "HEADLESS = True\n",
    "TIMEOUT_PAGE = 8\n",
    "# ===============================================\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  UC Driver (for LISTING only → 1 instance)\n",
    "# =====================================================\n",
    "def setup_uc_driver(headless=True):\n",
    "    opts = uc.ChromeOptions()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--window-size=1600,900\")\n",
    "    opts.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--log-level=3\")\n",
    "\n",
    "    try:\n",
    "        driver = uc.Chrome(options=opts)\n",
    "    except:\n",
    "        driver = setup_standard_driver(headless=headless)\n",
    "\n",
    "    driver.set_page_load_timeout(TIMEOUT_PAGE)\n",
    "    return driver\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  Standard Chrome Selenium (for DETAIL)\n",
    "# =====================================================\n",
    "def setup_standard_driver(headless=True):\n",
    "    try:\n",
    "        opts = ChromeOptions()\n",
    "        if headless:\n",
    "            opts.add_argument(\"--headless=new\")\n",
    "        opts.add_argument(\"--no-sandbox\")\n",
    "        opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "        opts.add_argument(\"--window-size=1600,900\")\n",
    "        opts.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "        opts.add_argument(\"--disable-gpu\")\n",
    "        opts.add_argument(\"--log-level=3\")\n",
    "\n",
    "        opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "        driver = webdriver.Chrome(options=opts)\n",
    "        driver.set_page_load_timeout(TIMEOUT_PAGE)\n",
    "        return driver\n",
    "    except:\n",
    "        return setup_uc_driver(headless=headless)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  Requests Session FastPool\n",
    "# =====================================================\n",
    "def create_fast_session():\n",
    "    s = requests.Session()\n",
    "    adapter = requests.adapters.HTTPAdapter(pool_connections=50, pool_maxsize=50, max_retries=2)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    return s\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  Kecamatan Code Reader\n",
    "# =====================================================\n",
    "def get_kode_kecamatan_from_json(nama_kecamatan, json_path=\"./list_kecamatan/kecamatan_kab_semarang.json\"):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    wilayah = next(iter(data))\n",
    "    return data[wilayah][\"kecamatan\"].get(nama_kecamatan)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  Extract UUID from referensi.data\n",
    "# =====================================================\n",
    "def extract_uuid_from_referensi(url, session):\n",
    "    try:\n",
    "        r = session.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        a = soup.find(\"a\", href=lambda x: x and \"profil-sekolah\" in x)\n",
    "        if a:\n",
    "            return a[\"href\"].rstrip(\"/\").split(\"/\")[-1]\n",
    "    except:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  Worker → Detail sekolah.data via Selenium\n",
    "# =====================================================\n",
    "def fetch_detail_worker(link_base_tuple):\n",
    "    link, base = link_base_tuple\n",
    "    session = create_fast_session()\n",
    "\n",
    "    uuid = extract_uuid_from_referensi(link, session)\n",
    "    if not uuid:\n",
    "        return base\n",
    "\n",
    "    url = f\"https://sekolah.data.kemendikdasmen.go.id/profil-sekolah/{uuid}\"\n",
    "\n",
    "    # Use standard selenium for parallel workers\n",
    "    driver = setup_standard_driver(headless=HEADLESS)\n",
    "\n",
    "    detail = {\n",
    "        \"Alamat\": \"-\",\n",
    "        \"Kepala Sekolah\": \"-\",\n",
    "        \"Telepon\": \"-\",\n",
    "        \"Email\": \"-\",\n",
    "        \"Website\": \"-\",\n",
    "        \"Jumlah Siswa Laki-laki\": \"-\",\n",
    "        \"Jumlah Siswa Perempuan\": \"-\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait Angular top part rendered\n",
    "        try:\n",
    "            WebDriverWait(driver, TIMEOUT_PAGE).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"h1 + p\"))\n",
    "            )\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "\n",
    "        # ========================================\n",
    "        # 1) Alamat\n",
    "        # ========================================\n",
    "        try:\n",
    "            detail[\"Alamat\"] = driver.find_element(By.CSS_SELECTOR, \"h1 + p\").text.strip()\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ========================================\n",
    "        # 2) Kepala Sekolah / Email / Telepon / Website\n",
    "        # ========================================\n",
    "        try:\n",
    "            blocks = driver.find_elements(By.CSS_SELECTOR, \"div.grid div.flex\")\n",
    "\n",
    "            for blk in blocks:\n",
    "                try:\n",
    "                    label = blk.find_element(By.CSS_SELECTOR, \".text-slate-500\").text.lower().strip()\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                if \"kepala sekolah\" in label:\n",
    "                    detail[\"Kepala Sekolah\"] = blk.find_element(By.CSS_SELECTOR, \".font-semibold\").text.strip()\n",
    "\n",
    "                elif \"telepon\" in label:\n",
    "                    try:\n",
    "                        detail[\"Telepon\"] = blk.find_element(By.TAG_NAME, \"a\").text.strip()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif \"email\" in label:\n",
    "                    try:\n",
    "                        detail[\"Email\"] = blk.find_element(By.TAG_NAME, \"a\").text.strip()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                elif \"website\" in label:\n",
    "                    try:\n",
    "                        detail[\"Website\"] = blk.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                    except:\n",
    "                        pass\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ========================================\n",
    "        # 3) Statistik Siswa FIX\n",
    "        # ========================================\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"section div.grid\"))\n",
    "            )\n",
    "            time.sleep(0.5)\n",
    "\n",
    "            stat_blocks = driver.find_elements(By.CSS_SELECTOR,\n",
    "                \"section div.grid div.flex, section div.grid > div\"\n",
    "            )\n",
    "\n",
    "            for sblk in stat_blocks:\n",
    "                try:\n",
    "                    lbl = sblk.find_element(By.CSS_SELECTOR, \"div.text-slate-600\").text.lower().strip()\n",
    "                    val = sblk.find_element(By.CSS_SELECTOR, \"div.text-2xl\").text.strip()\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                if (\"laki\" in lbl) or (\"lak\" in lbl) or (\"pa\" in lbl):\n",
    "                    detail[\"Jumlah Siswa Laki-laki\"] = val\n",
    "\n",
    "                if (\"perempuan\" in lbl) or (\"putri\" in lbl):\n",
    "                    detail[\"Jumlah Siswa Perempuan\"] = val\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"[STAT ERR]\", e)\n",
    "\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return {**base, **detail}\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  MAIN SCRAPER\n",
    "# =====================================================\n",
    "def get_sd_mi_schools_final(kode_kecamatan, nama_kecamatan, selected_fields, progress=None):\n",
    "\n",
    "    list_driver = setup_uc_driver(headless=HEADLESS)\n",
    "\n",
    "    sekolah_list = []\n",
    "    urls = []\n",
    "\n",
    "    need_detail = any(f in selected_fields for f in [\n",
    "        \"Alamat\", \"Kepala Sekolah\", \"Telepon\", \"Email\", \"Website\",\n",
    "        \"Jumlah Siswa Laki-laki\", \"Jumlah Siswa Perempuan\"\n",
    "    ])\n",
    "\n",
    "    # ---------------- LIST SEKOLAH ----------------\n",
    "    for jenjang, value in [(\"SD\", \"5\"), (\"MI\", \"9\")]:\n",
    "\n",
    "        url = f\"https://referensi.data.kemendikdasmen.go.id/pendidikan/dikdas/{kode_kecamatan}/3/all/{value}/all\"\n",
    "\n",
    "        list_driver.get(url)\n",
    "\n",
    "        WebDriverWait(list_driver, 20).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"table#table1 tbody tr\"))\n",
    "        )\n",
    "\n",
    "        # set 100 rows\n",
    "        try:\n",
    "            Select(list_driver.find_element(By.NAME, \"table1_length\")).select_by_value(\"100\")\n",
    "            time.sleep(0.3)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        rows = list_driver.find_elements(By.CSS_SELECTOR, \"table#table1 tbody tr\")\n",
    "\n",
    "        for r in rows:\n",
    "            data = {}\n",
    "            if \"Nama Sekolah\" in selected_fields:\n",
    "                data[\"Nama Sekolah\"] = r.find_element(By.CSS_SELECTOR, \"td:nth-child(3)\").text.strip()\n",
    "            if \"NPSN\" in selected_fields:\n",
    "                data[\"NPSN\"] = r.find_element(By.CSS_SELECTOR, \"td:nth-child(2)\").text.strip()\n",
    "            if \"Status\" in selected_fields:\n",
    "                data[\"Status\"] = r.find_element(By.CSS_SELECTOR, \"td:nth-child(6)\").text.strip()\n",
    "            if \"Kelurahan\" in selected_fields:\n",
    "                data[\"Kelurahan\"] = r.find_element(By.CSS_SELECTOR, \"td:nth-child(5)\").text.strip()\n",
    "\n",
    "            if need_detail:\n",
    "                urls.append((r.find_element(By.CSS_SELECTOR, \"a\").get_attribute(\"href\"), data))\n",
    "            else:\n",
    "                sekolah_list.append(data)\n",
    "\n",
    "    try:\n",
    "        list_driver.quit()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # ---------------- DETAIL (ThreadPool) ----------------\n",
    "    if need_detail and urls:\n",
    "\n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "            futures = [ex.submit(fetch_detail_worker, lb) for lb in urls]\n",
    "            for i, fut in enumerate(as_completed(futures)):\n",
    "                try:\n",
    "                    row = fut.result()\n",
    "                except:\n",
    "                    row = {}\n",
    "                results.append(row)\n",
    "\n",
    "                if progress and (i + 1) % 5 == 0:\n",
    "                    progress((i + 1) / len(futures))\n",
    "\n",
    "        sekolah_list.extend(results)\n",
    "\n",
    "    # SORT\n",
    "    if selected_fields:\n",
    "        key = selected_fields[0]\n",
    "        sekolah_list.sort(key=lambda x: str(x.get(key, \"\")).lower())\n",
    "\n",
    "    return sekolah_list\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  SAVE CSV\n",
    "# =====================================================\n",
    "def save_school_list_by_kecamatan(nama_kecamatan, selected_fields):\n",
    "    kode = get_kode_kecamatan_from_json(nama_kecamatan)\n",
    "    data = get_sd_mi_schools_final(kode, nama_kecamatan, selected_fields)\n",
    "    if not data:\n",
    "        return f\"Tidak ada data '{nama_kecamatan}'\", None\n",
    "\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    path = f\"output/list_sd_mi_{nama_kecamatan.lower().replace(' ', '_')}.csv\"\n",
    "\n",
    "    cols = [c for c in [\n",
    "        \"Kelurahan\", \"Nama Sekolah\", \"NPSN\", \"Status\",\n",
    "        \"Kepala Sekolah\", \"Alamat\", \"Telepon\", \"Email\", \"Website\",\n",
    "        \"Jumlah Siswa Laki-laki\", \"Jumlah Siswa Perempuan\"\n",
    "    ] if c in selected_fields]\n",
    "\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=cols)\n",
    "        w.writeheader()\n",
    "        w.writerows(data)\n",
    "\n",
    "    return f\"{len(data)} sekolah disimpan ke '{path}'\", path\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  Gradio UI\n",
    "# =====================================================\n",
    "import gradio as gr\n",
    "\n",
    "def run_scraper(nama_kecamatan, selected_fields):\n",
    "    if nama_kecamatan in [\"\", \"-- Pilih Kecamatan --\"]:\n",
    "        yield \"Pilih kecamatan.\", gr.update(visible=False)\n",
    "        return\n",
    "\n",
    "    if not selected_fields:\n",
    "        yield \"Pilih minimal 1 kolom.\", gr.update(visible=False)\n",
    "        return\n",
    "\n",
    "    t0 = time.time()\n",
    "    s, p = save_school_list_by_kecamatan(nama_kecamatan, selected_fields)\n",
    "    d = int(time.time() - t0)\n",
    "    m, s_ = divmod(d, 60)\n",
    "    txt = f\"{s}\\nWaktu: {m} menit {s_} detik\" if m else f\"{s}\\nWaktu: {s_} detik\"\n",
    "\n",
    "    yield txt, gr.update(value=p, visible=True)\n",
    "\n",
    "\n",
    "def create_gradio_ui(json_path=\"./list_kecamatan/kecamatan_kab_semarang.json\"):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    wilayah = next(iter(data))\n",
    "    kec_list = sorted(data[wilayah][\"kecamatan\"].keys())\n",
    "\n",
    "    fields = [\n",
    "        \"Kelurahan\", \"Nama Sekolah\", \"NPSN\", \"Status\",\n",
    "        \"Kepala Sekolah\", \"Alamat\", \"Telepon\", \"Email\", \"Website\",\n",
    "        \"Jumlah Siswa Laki-laki\", \"Jumlah Siswa Perempuan\"\n",
    "    ]\n",
    "\n",
    "    with gr.Blocks(title=\"Scraper SD/MI Semarang — Selenium Full (dengan ThreadPool)\") as demo:\n",
    "        gr.Markdown(\"## Scraper SD/MI Kab. Semarang — Selenium Full (dengan ThreadPool)\")\n",
    "\n",
    "        kec = gr.Dropdown(label=\"Pilih Kecamatan\",\n",
    "                          choices=[\"-- Pilih Kecamatan --\"] + kec_list,\n",
    "                          value=\"-- Pilih Kecamatan --\")\n",
    "\n",
    "        kolom = gr.CheckboxGroup(label=\"Kolom CSV\", choices=fields)\n",
    "\n",
    "        btn = gr.Button(\"Mulai Scrape\")\n",
    "        stat = gr.Textbox(label=\"Status\", lines=4)\n",
    "        file = gr.File(label=\"File CSV\", visible=False)\n",
    "\n",
    "        btn.click(run_scraper, [kec, kolom], [stat, file])\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ui = create_gradio_ui()\n",
    "    ui.launch(inbrowser=True, share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a069eb76",
   "metadata": {},
   "source": [
    "## UPDATE FINAL, SUMBER DATA LISTING DIUBAH KARENA WEBSITE REFERENSI KEMENDIKDASEM ERROR/SUDAH TIDAK ADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51df6680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 09:46:27,836 [INFO] HTTP Request: GET http://127.0.0.1:7863/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 09:46:27,845 [INFO] HTTP Request: HEAD http://127.0.0.1:7863/ \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 09:46:27,845 [INFO] HTTP Request: HEAD https://huggingface.co/api/telemetry/https%3A/api.gradio.app/gradio-initiated-analytics \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-25 09:46:28,125 [INFO] HTTP Request: HEAD https://huggingface.co/api/telemetry/https%3A/api.gradio.app/gradio-launched-telemetry \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 09:46:28,479 [INFO] HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2025-11-25 09:46:41,014 [INFO] patching driver executable C:\\Users\\ASUS\\appdata\\roaming\\undetected_chromedriver\\undetected_chromedriver.exe\n",
      "2025-11-25 09:46:43,705 [INFO] [LISTING] Attempt 1/4: https://dapo.kemendikdasmen.go.id/sp/3/032211\n",
      "2025-11-25 09:46:56,079 [WARNING] [LISTING] Tabel belum muncul, refresh halaman...\n",
      "2025-11-25 09:47:06,345 [INFO] [LISTING] Attempt 2/4: https://dapo.kemendikdasmen.go.id/sp/3/032211\n",
      "2025-11-25 09:47:19,085 [INFO] [LISTING] Tabel ditemukan: table#dataTables tbody tr\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Selenium imports (standard + fallback)\n",
    "try:\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    STANDARD_SELENIUM_AVAILABLE = True\n",
    "except Exception:\n",
    "    STANDARD_SELENIUM_AVAILABLE = False\n",
    "\n",
    "import undetected_chromedriver as uc\n",
    "uc.logger.setLevel(logging.ERROR)\n",
    "\n",
    "# =================== CONFIG ===================\n",
    "MAX_WORKERS = 12\n",
    "HEADLESS = True\n",
    "TIMEOUT_PAGE = 20  \n",
    "# ===============================================\n",
    "\n",
    "# =================== Logging ===================\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "log = logging.getLogger(__name__)\n",
    "# ===============================================\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  DRIVER LISTING → UC CHROME\n",
    "# =====================================================\n",
    "def setup_uc_driver(headless=True):\n",
    "    opts = uc.ChromeOptions()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--window-size=1600,900\")\n",
    "    opts.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "    opts.add_argument(\"--disable-gpu\")\n",
    "    opts.add_argument(\"--log-level=3\")\n",
    "\n",
    "    try:\n",
    "        driver = uc.Chrome(options=opts)\n",
    "    except Exception:\n",
    "        driver = setup_standard_driver(headless)\n",
    "\n",
    "    driver.set_page_load_timeout(TIMEOUT_PAGE)\n",
    "    return driver\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  DRIVER DETAIL → STANDARD SELENIUM\n",
    "# =====================================================\n",
    "def setup_standard_driver(headless=True):\n",
    "    try:\n",
    "        opts = ChromeOptions()\n",
    "        if headless:\n",
    "            opts.add_argument(\"--headless=new\")\n",
    "        opts.add_argument(\"--no-sandbox\")\n",
    "        opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "        opts.add_argument(\"--window-size=1600,900\")\n",
    "        opts.add_argument(\"--blink-settings=imagesEnabled=false\")\n",
    "        opts.add_argument(\"--disable-gpu\")\n",
    "\n",
    "        # anti-automation\n",
    "        opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "        driver = webdriver.Chrome(options=opts)\n",
    "        driver.set_page_load_timeout(TIMEOUT_PAGE)\n",
    "        return driver\n",
    "\n",
    "    except Exception as e:\n",
    "        log.warning(f\"Standard Chrome gagal, fallback ke UC: {e}\")\n",
    "        return setup_uc_driver(headless)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  BACA JSON KODE KECAMATAN\n",
    "# =====================================================\n",
    "def get_kode_kecamatan_from_json(nama_kecamatan, json_path=\"./list_kecamatan/kecamatan_kab_semarang.json\"):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    wilayah = next(iter(data))\n",
    "    return data[wilayah][\"kecamatan\"].get(nama_kecamatan)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  AUTO-DETECT TABEL DAPO SP\n",
    "# =====================================================\n",
    "def wait_for_table_rows(driver, timeout=20):\n",
    "\n",
    "    # ID tabel sering berubah-ubah\n",
    "    TABLE_CHOICES = [\n",
    "        \"table#dataTables tbody tr\",\n",
    "        \"table#example tbody tr\",\n",
    "        \"table#myTable tbody tr\",\n",
    "        \"table.table tbody tr\",\n",
    "        \"tbody tr\"\n",
    "    ]\n",
    "\n",
    "    end = time.time() + timeout\n",
    "    while time.time() < end:\n",
    "        for sel in TABLE_CHOICES:\n",
    "            try:\n",
    "                rows = driver.find_elements(By.CSS_SELECTOR, sel)\n",
    "                if rows and len(rows) > 0:\n",
    "                    return sel\n",
    "            except:\n",
    "                pass\n",
    "        time.sleep(0.25)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  AUTO RETRY + AUTO REFRESH UNTUK LISTING DAPO\n",
    "# =====================================================\n",
    "def load_listing_with_retry(driver, url, max_retry=4):\n",
    "    \"\"\"\n",
    "    Mekanisme super stabil:\n",
    "    - get halaman\n",
    "    - deteksi tabel\n",
    "    - kalau gagal → refresh → retry\n",
    "    - work untuk dapo yang lemot / blank\n",
    "    \"\"\"\n",
    "\n",
    "    for attempt in range(1, max_retry + 1):\n",
    "        log.info(f\"[LISTING] Attempt {attempt}/{max_retry}: {url}\")\n",
    "\n",
    "        try:\n",
    "            driver.get(url)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        selector = wait_for_table_rows(driver, timeout=12)\n",
    "        if selector:\n",
    "            log.info(f\"[LISTING] Tabel ditemukan: {selector}\")\n",
    "            return selector\n",
    "\n",
    "        log.warning(\"[LISTING] Tabel belum muncul, refresh halaman...\")\n",
    "        try:\n",
    "            driver.refresh()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        time.sleep(2)  # tunggu server cooldown\n",
    "\n",
    "    log.error(\"[LISTING] Gagal memuat tabel setelah semua retry\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  FETCH DETAIL SEKOLAH.DATA (FINAL & STABIL)\n",
    "# =====================================================\n",
    "def fetch_detail_worker(npsn_and_base):\n",
    "    npsn, base = npsn_and_base\n",
    "    driver = None\n",
    "\n",
    "    detail = {\n",
    "        \"Alamat\": \"-\",\n",
    "        \"Kepala Sekolah\": \"-\",\n",
    "        \"Telepon\": \"-\",\n",
    "        \"Email\": \"-\",\n",
    "        \"Website\": \"-\",\n",
    "        \"Akreditasi\": \"-\",\n",
    "        \"Yayasan\": \"-\",\n",
    "        \"Jumlah Siswa Laki-laki\": \"-\",\n",
    "        \"Jumlah Siswa Perempuan\": \"-\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        driver = setup_standard_driver(headless=HEADLESS)\n",
    "\n",
    "        # 1. buka pencarian\n",
    "        search_url = f\"https://sekolah.data.kemendikdasmen.go.id/sekolah?keyword={npsn}&page=0&size=12\"\n",
    "        driver.get(search_url)\n",
    "\n",
    "        # tunggu grid hasil\n",
    "        try:\n",
    "            WebDriverWait(driver, 12).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//article\"))\n",
    "            )\n",
    "        except:\n",
    "            return base\n",
    "\n",
    "        # cari article yg mengandung NPSN\n",
    "        articles = driver.find_elements(By.XPATH, \"//article\")\n",
    "        target = None\n",
    "        for a in articles:\n",
    "            if str(npsn) in a.text:\n",
    "                target = a\n",
    "                break\n",
    "\n",
    "        if target is None:\n",
    "            target = articles[0]\n",
    "\n",
    "        # KLIK tombol Lihat\n",
    "        try:\n",
    "            lihat_btn = target.find_element(By.XPATH, \".//button[contains(.,'Lihat')]\")\n",
    "            driver.execute_script(\"arguments[0].click();\", lihat_btn)\n",
    "        except:\n",
    "            return base\n",
    "\n",
    "        # Tunggu halaman detail stabil\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//h1\"))\n",
    "        )\n",
    "\n",
    "        # \"patch sakti\" supaya Angular render semua\n",
    "        time.sleep(1.3)\n",
    "\n",
    "        # =============================== ALAMAT ===============================\n",
    "        try:\n",
    "            h1 = driver.find_element(By.XPATH, \"//h1\")\n",
    "            alamat = h1.find_element(By.XPATH, \"following-sibling::p[1]\").text.strip()\n",
    "            detail[\"Alamat\"] = alamat\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # =============================== INFO GRID ===============================\n",
    "        info_blocks = driver.find_elements(\n",
    "            By.XPATH,\n",
    "            \"//div[contains(@class,'grid') and contains(@class,'gap-x-6')]//div[contains(@class,'flex')]\"\n",
    "        )\n",
    "\n",
    "        for blk in info_blocks:\n",
    "            try:\n",
    "                # ambil label\n",
    "                label = blk.find_element(\n",
    "                    By.XPATH,\n",
    "                    \".//div[contains(@class,'text-slate-500')]\"\n",
    "                ).text.lower().strip()\n",
    "\n",
    "                # ambil isi default dari <div class='font-semibold'>\n",
    "                try:\n",
    "                    value_div = blk.find_element(\n",
    "                        By.XPATH,\n",
    "                        \".//div[contains(@class,'font-semibold')]\"\n",
    "                    ).text.strip()\n",
    "                except:\n",
    "                    value_div = \"\"\n",
    "\n",
    "                # ================== AKREDITASI ==================\n",
    "                if \"akreditasi\" in label:\n",
    "                    detail[\"Akreditasi\"] = value_div\n",
    "\n",
    "                # ================== KEPALA SEKOLAH ==================\n",
    "                elif \"kepala sekolah\" in label:\n",
    "                    detail[\"Kepala Sekolah\"] = value_div\n",
    "\n",
    "                # ================== YAYASAN ==================\n",
    "                elif \"yayasan\" in label:\n",
    "                    detail[\"Yayasan\"] = value_div\n",
    "\n",
    "                # ================== TELEPON ==================\n",
    "                elif \"telepon\" in label:\n",
    "                    try:\n",
    "                        detail[\"Telepon\"] = blk.find_element(\n",
    "                            By.XPATH, \".//a[starts-with(@href,'tel')]\"\n",
    "                        ).text.strip()\n",
    "                    except:\n",
    "                        detail[\"Telepon\"] = value_div or \"-\"\n",
    "\n",
    "                # ================== EMAIL ==================\n",
    "                elif \"email\" in label:\n",
    "                    try:\n",
    "                        detail[\"Email\"] = blk.find_element(\n",
    "                            By.XPATH, \".//a[starts-with(@href,'mailto')]\"\n",
    "                        ).text.strip()\n",
    "                    except:\n",
    "                        detail[\"Email\"] = value_div or \"-\"\n",
    "\n",
    "                # ================== WEBSITE ==================\n",
    "                elif \"website\" in label:\n",
    "                    try:\n",
    "                        href = blk.find_element(By.XPATH, \".//a\").get_attribute(\"href\")\n",
    "                        detail[\"Website\"] = href if href.startswith(\"http\") else \"-\"\n",
    "                    except:\n",
    "                        detail[\"Website\"] = \"-\"\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # =============================== STATISTIK SISWA ===============================\n",
    "        stat_blocks = driver.find_elements(\n",
    "            By.XPATH,\n",
    "            \"//div[contains(@class,'rounded-xl')]//div[contains(@class,'px-3')]\"\n",
    "        )\n",
    "\n",
    "        for s in stat_blocks:\n",
    "            try:\n",
    "                lbl = s.find_element(By.XPATH, \".//div[contains(@class,'line-clamp-1')]\").text.lower()\n",
    "                val = s.find_element(By.XPATH, \".//div[contains(@class,'text-2xl')]\").text.strip()\n",
    "\n",
    "                if \"laki\" in lbl:\n",
    "                    detail[\"Jumlah Siswa Laki-laki\"] = val\n",
    "\n",
    "                elif \"perempuan\" in lbl:\n",
    "                    detail[\"Jumlah Siswa Perempuan\"] = val\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        return {**base, **detail}\n",
    "\n",
    "    except Exception as e:\n",
    "        log.exception(f\"DETAIL ERROR {npsn}: {e}\")\n",
    "        return base\n",
    "\n",
    "    finally:\n",
    "        if driver:\n",
    "            try:\n",
    "                driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  MAIN SCRAPER LISTING\n",
    "# =====================================================\n",
    "def get_sd_mi_schools_final(kode_kecamatan, nama_kecamatan, selected_fields, progress=None):\n",
    "\n",
    "    list_driver = None\n",
    "    sekolah_list = []\n",
    "    urls = []\n",
    "\n",
    "    need_detail = any(f in selected_fields for f in [\n",
    "        \"Alamat\", \"Kepala Sekolah\", \"Telepon\", \"Email\",\n",
    "        \"Website\", \"Jumlah Siswa Laki-laki\",\n",
    "        \"Jumlah Siswa Perempuan\", \"Akreditasi\", \"Yayasan\"\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        list_driver = setup_uc_driver(headless=HEADLESS)\n",
    "\n",
    "        if not kode_kecamatan:\n",
    "            return []\n",
    "\n",
    "        url = f\"https://dapo.kemendikdasmen.go.id/sp/3/{kode_kecamatan}\"\n",
    "\n",
    "        # === LOAD HALAMAN + AUTO RETRY ===\n",
    "        selector = load_listing_with_retry(list_driver, url, max_retry=4)\n",
    "        if not selector:\n",
    "            return []\n",
    "\n",
    "        rows = list_driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "\n",
    "        for r in rows:\n",
    "            cols = r.find_elements(By.TAG_NAME, \"td\")\n",
    "            if len(cols) < 5:\n",
    "                continue\n",
    "\n",
    "            data = {}\n",
    "\n",
    "            if \"Nama Sekolah\" in selected_fields:\n",
    "                data[\"Nama Sekolah\"] = cols[1].text.strip()\n",
    "\n",
    "            if \"NPSN\" in selected_fields:\n",
    "                data[\"NPSN\"] = cols[2].text.strip()\n",
    "\n",
    "            if \"Status\" in selected_fields:\n",
    "                data[\"Status\"] = cols[4].text.strip()\n",
    "\n",
    "            if need_detail:\n",
    "                urls.append((cols[2].text.strip(), data))\n",
    "            else:\n",
    "                sekolah_list.append(data)\n",
    "\n",
    "    except Exception as e:\n",
    "        log.exception(f\"LISTING ERROR {nama_kecamatan}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if list_driver:\n",
    "            try:\n",
    "                list_driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    # === DETAIL MULTI THREAD ===\n",
    "    if need_detail and urls:\n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "            futures = [ex.submit(fetch_detail_worker, u) for u in urls]\n",
    "            for i, fut in enumerate(as_completed(futures)):\n",
    "                try:\n",
    "                    results.append(fut.result())\n",
    "                except:\n",
    "                    results.append({})\n",
    "        sekolah_list.extend(results)\n",
    "\n",
    "    return sekolah_list\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  SAVE CSV\n",
    "# =====================================================\n",
    "def save_school_list_by_kecamatan(nama_kecamatan, selected_fields):\n",
    "    kode = get_kode_kecamatan_from_json(nama_kecamatan)\n",
    "    data = get_sd_mi_schools_final(kode, nama_kecamatan, selected_fields)\n",
    "\n",
    "    if not data:\n",
    "        return f\"Tidak ada data '{nama_kecamatan}'\", None\n",
    "\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "    path = f\"output/list_sd_mi_{nama_kecamatan.lower().replace(' ', '_')}.csv\"\n",
    "\n",
    "    cols = selected_fields.copy()\n",
    "\n",
    "    with open(path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=cols)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    return f\"{len(data)} sekolah disimpan ke '{path}'\", path\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "#  Gradio UI\n",
    "# =====================================================\n",
    "import gradio as gr\n",
    "\n",
    "def run_scraper(nama_kecamatan, selected_fields):\n",
    "    # quick validation\n",
    "    if nama_kecamatan in [\"\", \"-- Pilih Kecamatan --\"]:\n",
    "        yield \"Pilih kecamatan.\", gr.update(visible=False)\n",
    "        return\n",
    "\n",
    "    if not selected_fields:\n",
    "        yield \"Pilih minimal 1 kolom.\", gr.update(visible=False)\n",
    "        return\n",
    "\n",
    "    t0 = time.time()\n",
    "    s, p = save_school_list_by_kecamatan(nama_kecamatan, selected_fields)\n",
    "    d = int(time.time() - t0)\n",
    "    m, s_ = divmod(d, 60)\n",
    "    txt = f\"{s}\\nWaktu: {m} menit {s_} detik\" if m else f\"{s}\\nWaktu: {s_} detik\"\n",
    "\n",
    "    yield txt, gr.update(value=p, visible=True)\n",
    "\n",
    "\n",
    "def create_gradio_ui(json_path=\"./list_kecamatan/kecamatan_kab_semarang.json\"):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    wilayah = next(iter(data))\n",
    "    kec_list = sorted(data[wilayah][\"kecamatan\"].keys())\n",
    "\n",
    "    fields = [\n",
    "        \"Nama Sekolah\", \"NPSN\", \"Status\",\n",
    "        \"Kepala Sekolah\", \"Alamat\", \"Telepon\", \"Email\", \"Website\",\n",
    "        \"Jumlah Siswa Laki-laki\", \"Jumlah Siswa Perempuan\", \"Akreditasi\", \"Yayasan\"\n",
    "    ]\n",
    "\n",
    "    with gr.Blocks(title=\"Scraper SD/MI Semarang — Selenium Full (UPDATED) with ThreadPool\") as demo:\n",
    "        gr.Markdown(\"## Scraper SD/MI Kab. Semarang — Selenium Full (dapo -> sekolah.data) with ThreadPool\")\n",
    "\n",
    "        kec = gr.Dropdown(label=\"Pilih Kecamatan\",\n",
    "                          choices=[\"-- Pilih Kecamatan --\"] + kec_list,\n",
    "                          value=\"-- Pilih Kecamatan --\")\n",
    "\n",
    "        kolom = gr.CheckboxGroup(label=\"Kolom CSV\", choices=fields)\n",
    "\n",
    "        btn = gr.Button(\"Mulai Scrape\")\n",
    "        stat = gr.Textbox(label=\"Status\", lines=4)\n",
    "        file = gr.File(label=\"File CSV\", visible=False)\n",
    "\n",
    "        btn.click(run_scraper, [kec, kolom], [stat, file])\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ui = create_gradio_ui()\n",
    "    ui.launch(inbrowser=True, share=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
